{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bleu\n",
    "import weighted_ngram_match\n",
    "import syntax_match\n",
    "import dataflow_match\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "lang = \"python\"\n",
    "alpha,beta,gamma,theta = [0.1, 0.1, 0.4, 0.4]\n",
    "\n",
    "for path, subdirs, files in os.walk(\"CodeContestsCleanCodeBLEU\"):\n",
    "    for filename in files:\n",
    "        df = pd.read_csv(os.path.join(path, filename))\n",
    "        scores = []\n",
    "        for index, row in df.iterrows():\n",
    "            if row[\"Solution Language\"] != \"PYTHON3\":\n",
    "                scores.append(\"\")\n",
    "                continue\n",
    "            elif pd.isnull(row[\"Incorrect Solution\"]):\n",
    "                scores.append(1)\n",
    "                continue\n",
    "            else:\n",
    "                # preprocess inputs\n",
    "                reference_code = df[(df['Description'] == row[\"Description\"]) & \n",
    "                                    (df[\"Correct Solution\"].notnull()) &\n",
    "                                    (df[\"Solution Language\"] == \"PYTHON3\")][\"Correct Solution\"].tolist() \n",
    "                pre_references = [[reference] \\\n",
    "                                for reference in reference_code]\n",
    "                hypothesis = [row[\"Incorrect Solution\"]]\n",
    "\n",
    "                #We will exclude incorrect solutions to problems with no correct solutions\n",
    "                if len(reference_code) == 0:\n",
    "                    scores.append(\"\")\n",
    "                    continue\n",
    "\n",
    "                #We will take at most 15 reference solutions due to time constraints\n",
    "                pre_references = pre_references[:15]    \n",
    "                references = []\n",
    "                for i in range(len(hypothesis)):\n",
    "                    ref_for_instance = []\n",
    "                    for j in range(len(pre_references)):\n",
    "                        ref_for_instance.append(pre_references[j][i])\n",
    "                    references.append(ref_for_instance)\n",
    "\n",
    "\n",
    "                # calculate ngram match (BLEU)\n",
    "                tokenized_hyps = [x.split() for x in hypothesis]\n",
    "                tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "                ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "\n",
    "                # calculate weighted ngram match\n",
    "                keywords = [x.strip() for x in open('keywords/'+lang+'.txt', 'r', encoding='utf-8').readlines()]\n",
    "                def make_weights(reference_tokens, key_word_list):\n",
    "                    return {token:1 if token in key_word_list else 0.2 \\\n",
    "                            for token in reference_tokens}\n",
    "                tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                            for reference_tokens in reference] for reference in tokenized_refs]\n",
    "                weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "                # calculate syntax match\n",
    "                syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis, lang)\n",
    "\n",
    "                # calculate dataflow match\n",
    "                dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis, lang)\n",
    "\n",
    "                code_bleu_score = alpha*ngram_match_score\\\n",
    "                                + beta*weighted_ngram_match_score\\\n",
    "                                + gamma*syntax_match_score\\\n",
    "                                + theta*dataflow_match_score\n",
    "\n",
    "                scores.append(code_bleu_score)\n",
    "\n",
    "        df[\"Score\"] = scores\n",
    "        df.to_csv(os.path.join(path, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214efa21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
